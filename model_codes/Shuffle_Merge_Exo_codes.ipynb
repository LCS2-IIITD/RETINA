{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dynamic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "name_ext = input(\"Enter the file-name identifier extension\")\n",
    "if len(name_ext) == 0:\n",
    "    name_ext = \"53\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part1_1_train_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list1 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part1_2_train_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list2 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part2_1_train_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list3 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part2_2_train_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list4 = pickle.load(handle)\n",
    "new_list = list1 + list2 + list3 + list4\n",
    "\n",
    "train = np.asarray(new_list)\n",
    "print(train.shape)\n",
    "\n",
    "nx, ny, _, _ = train.shape()\n",
    "h5f = h5py.File(\n",
    "    os.path.join(\n",
    "        data, 'train_dynamic_neural_' + str(nx) + '_' + str(ny) + '_' +\n",
    "        name_ext + '.h5'), 'w')\n",
    "h5f.create_dataset('dataset_1', data=train)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part1_1_test_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list1 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part1_2_test_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list2 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part2_1_test_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list3 = pickle.load(handle)\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data_path,\n",
    "            'dyamic_neural_merged_part2_2_test_' + name_ext + '.pickle'),\n",
    "        'rb') as handle:\n",
    "    list4 = pickle.load(handle)\n",
    "new_list = list1 + list2 + list3 + list4\n",
    "\n",
    "test = np.asarray(new_list)\n",
    "print(test.shape)\n",
    "\n",
    "nx, ny, _, _ = test.shape()\n",
    "h5f = h5py.File(\n",
    "    os.path.join(\n",
    "        data, 'test_dynamic_neural_' + str(nx) + '_' + str(ny) + '_' +\n",
    "        name_ext + '.h5'), 'w')\n",
    "h5f.create_dataset('dataset_1', data=test)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "name_ext = input(\"Enter the file-name identifier extension\")\n",
    "if len(name_ext) == 0:\n",
    "    name_ext = \"30h_15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\n",
    "    os.path.join(data, 'train_static_flat_3057_200_' + name_ext + 'h5'), 'r')\n",
    "train = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "with open(os.path.join(data, 'train_static_label_3057_200_30.pickle'),\n",
    "          'rb') as handle:\n",
    "    l = pickle.load(handle)\n",
    "\n",
    "new_train = []\n",
    "new_train_labels = []\n",
    "c = 0\n",
    "for row in range(len(l)):\n",
    "    inner_l = []\n",
    "    inner_lt = []\n",
    "    inner = []\n",
    "    num = random.sample(range(0, len(l[row])), len(l[row]))\n",
    "    for i in num:\n",
    "        inner.append(train[row][i])\n",
    "        inner_l.append(l[row][i])\n",
    "    new_train.append(inner)\n",
    "    new_train_labels.append(inner_l)\n",
    "\n",
    "train = np.asarray(new_train)\n",
    "l = np.asarray(new_train_labels).reshape(3057, 200,\n",
    "                                         1)  #(x,y,z,1) form for dynamic\n",
    "\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data,\n",
    "            'static_neural_labels_temp_shuffle_new_' + name_ext + '.pickle'),\n",
    "        'wb') as handle:\n",
    "    pickle.dump(l, handle)\n",
    "\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data,\n",
    "                 'train_static_neural_shuffle_3057_200_' + name_ext + '.h5'),\n",
    "    'w')\n",
    "h5f.create_dataset('dataset_1', data=train)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\n",
    "    os.path.join(data, 'test_static_flat_765_200_' + name_ext + '.h5'), 'r')\n",
    "test = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "\n",
    "with open(os.path.join(data, 'test_static_label_765_200_30.pickle'),\n",
    "          'rb') as handle:\n",
    "    l_test = pickle.load(handle)\n",
    "\n",
    "new_test = []\n",
    "new_test_labels = []\n",
    "for row in range(len(l_test)):\n",
    "    inner_l = []\n",
    "    inner = []\n",
    "    num = random.sample(range(0, len(l_test[row])), len(l_test[row]))\n",
    "    for i in num:\n",
    "        inner.append(test[row][i])\n",
    "        inner_l.append(l_test[row][i])\n",
    "    new_test.append(inner)\n",
    "    new_test_labels.append(inner_l)\n",
    "\n",
    "test = np.asarray(new_test)\n",
    "l_test = np.asarray(new_test_labels).reshape(765, 500, 1)\n",
    "\n",
    "with open(\n",
    "        os.path.join(\n",
    "            data, 'static_neural_labels_temp_test_shuffle_new_' + name_ext +\n",
    "            '.pickle'), 'wb') as handle:\n",
    "    pickle.dump(l_test, handle)\n",
    "\n",
    "import h5py\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data,\n",
    "                 'test_static_neural_shuffle_765_500_' + name_ext + '.h5'),\n",
    "    'w')\n",
    "h5f.create_dataset('dataset_1', data=test)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static_Exgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "news_size = input(\"Enter the ablation news size either 15/30/45/60\")\n",
    "if not news_size:\n",
    "    news_size = 15\n",
    "split_at = news_size * 500  #news doc2vec is 500 dim each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train static + news vector\n",
    "h5f = h5py.File(os.path.join(data_path, 'train_static_neural_3057_200_30.h5'),\n",
    "                'r')\n",
    "train = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "with open(os.path.join(data_path, \"news2vec_static_train.npy\"), \"rb\") as f:\n",
    "    news2_vec_train = np.load(f)\n",
    "\n",
    "train_new = []\n",
    "ir, jr, _ = train.shape\n",
    "for i in range(ir):\n",
    "    train_temp = train[i]\n",
    "    news_temp_flat = news2_vec_train[i].flatten()[:news_size]\n",
    "    train_per_row = []\n",
    "    for j in range(jr):\n",
    "        train_ex = np.concatenate([train_temp[j], news_temp_flat])\n",
    "        train_per_row.append(train_ex)\n",
    "    train_new.append(train_per_row)\n",
    "\n",
    "train = np.asarray(train_new)\n",
    "\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data, 'train_static_flat_3057_200_30h_' + news_size + '.h5'),\n",
    "    'w')\n",
    "h5f.create_dataset('dataset_1', data=train)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test static + news vector\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data_path,\n",
    "                 'test_static_neural_765_500_30' + news_size + '.h5'), 'r')\n",
    "test = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "with open(os.path.join(data_path, \"news2vec_static_test.npy\"), \"rb\") as f:\n",
    "    news2_vec_test = np.load(f)\n",
    "\n",
    "test_new = []\n",
    "ir, jr, _ = test.shape\n",
    "for i in range(ir):\n",
    "    test_temp = test[i]\n",
    "    test_news_temp_flat = news2_vec_test[i].flatten()[:news_size]\n",
    "    test_per_row = []\n",
    "    for j in range(jr):\n",
    "        test_ex = np.concatenate([test_temp[j], test_news_temp_flat])\n",
    "        test_per_row.append(test_ex)\n",
    "    test_new.append(test_per_row)\n",
    "\n",
    "test = train = np.asarray(test_new)\n",
    "\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data_path,\n",
    "                 'test_static_flat_765_200_30h' + news_size + '.h5'), 'w')\n",
    "h5f.create_dataset('dataset_1', data=test)\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
