{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dHowJjhOAZt6",
    "outputId": "e26602b1-00ea-4222-cfdf-bd61c8af8621"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jfu5PtY-Atm3",
    "outputId": "be86691a-fe12-4ea4-a915-f7ceda2e3a92"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import h5py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation_feature = 'history or max_feature_length or no_exo'\n",
    "# ablation_value = # 10,20,30,50,100 for history or 300,600,1000 for max_feature_length\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "name_ext = input(\"Enter the file-name identifier extension\")\n",
    "#eg 30h for use_history size=30.\n",
    "if len(name_ext) == 0:\n",
    "    name_ext = \"30h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETINA MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mIdPiJtA6BC"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim=64, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, q, k, v):\n",
    "        q = tf.expand_dims(q, axis=1)\n",
    "        print(q)\n",
    "        att_weights = tf.squeeze(tf.matmul(k, q, transpose_b=True),\n",
    "                                 axis=-1) / tf.sqrt(\n",
    "                                     tf.cast(self.hidden_dim, tf.float32))\n",
    "        att_weights = tf.expand_dims(tf.nn.softmax(att_weights, axis=-1),\n",
    "                                     axis=-1)\n",
    "        v = tf.expand_dims(v, axis=1)\n",
    "        return tf.reduce_sum(v * att_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_new(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim=64, **kwargs):\n",
    "        super(Attention_new, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Attention_new, self).build(input_shape)\n",
    "\n",
    "    def call(self, q, k, v):\n",
    "        q = tf.expand_dims(q, axis=1)\n",
    "        att_weights = tf.squeeze(tf.matmul(k, q, transpose_b=True),\n",
    "                                 axis=-1) / tf.sqrt(\n",
    "                                     tf.cast(self.hidden_dim, tf.float32))\n",
    "        att_weights = tf.expand_dims(tf.nn.softmax(att_weights, axis=-1),\n",
    "                                     axis=-1)\n",
    "        v = v * att_weights\n",
    "        return tf.reduce_sum(v, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqS3jfBhH7Yz"
   },
   "outputs": [],
   "source": [
    "def getAttention(hidden_dim=64, bert_dim=500, tweet_dim=50, **kwargs):\n",
    "    '''tf.keras model implementing attention.\n",
    "    Variable 'mask' denotes positions of the news sequence to be\n",
    "    mased (due to zero-padding, etc.). Example of mask: [0, 0, 0, 1] \n",
    "    which will mask out the last sequence element.'''\n",
    "\n",
    "    news_inp = tf.keras.layers.Input(shape=(None, bert_dim))\n",
    "    tweet_inp = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    q_tweet = tf.keras.layers.Dense(hidden_dim, use_bias=False)(tweet_inp)\n",
    "    k_news = tf.keras.layers.Dense(hidden_dim, use_bias=False)(news_inp)\n",
    "    output = Attention()(q_tweet, k_news, tweet_inp)\n",
    "    return tf.keras.models.Model([tweet_inp, news_inp], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttention_new(hidden_dim=64, bert_dim=500, tweet_dim=50, **kwargs):\n",
    "    '''tf.keras model implementing attention.\n",
    "    Variable 'mask' denotes positions of the news sequence to be\n",
    "    mased (due to zero-padding, etc.). Example of mask: [0, 0, 0, 1] \n",
    "    which will mask out the last sequence element.'''\n",
    "\n",
    "    news_inp = tf.keras.layers.Input(shape=(None, bert_dim))\n",
    "    tweet_inp = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    q_tweet = tf.keras.layers.Dense(hidden_dim, use_bias=False)(tweet_inp)\n",
    "    k_news = tf.keras.layers.Dense(hidden_dim, use_bias=False)(news_inp)\n",
    "    v_news = tf.keras.layers.Dense(hidden_dim, use_bias=False)(news_inp)\n",
    "    output = Attention_new()(q_tweet, k_news, v_news)\n",
    "    return tf.keras.models.Model([tweet_inp, news_inp], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETINA Static Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nDBzxuhgeMa"
   },
   "outputs": [],
   "source": [
    "def StaticModel(hidden_dim=64,\n",
    "                bert_dim=500,\n",
    "                tweet_dim=50,\n",
    "                feature_dim=616,\n",
    "                num_users=500,\n",
    "                dropout=0.3,\n",
    "                att_mode='tweet',\n",
    "                **kwargs):\n",
    "    '''Creates static prediction model. \n",
    "    'num_users' is the largest number of users can be taken (retweeters+followers). \n",
    "    Prediction is done every time step independently. \n",
    "    'feature_dim' is the unnormalized features (used previously for logreg).'''\n",
    "\n",
    "    features = tf.keras.layers.Input(shape=(num_users, feature_dim))\n",
    "    exo_signal = tf.keras.layers.Input(shape=(None, bert_dim))\n",
    "    root_tweet = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    norm_feature = tf.keras.layers.LayerNormalization()(features)\n",
    "    int_feature = tf.keras.layers.Dense(hidden_dim,\n",
    "                                        activation='tanh')(norm_feature)\n",
    "    int_feature = tf.keras.layers.Dropout(dropout)(int_feature)\n",
    "\n",
    "    if att_mode == 'tweet':\n",
    "        att_root = getAttention(hidden_dim=hidden_dim,\n",
    "                                bert_dim=bert_dim,\n",
    "                                tweet_dim=tweet_dim)([root_tweet, exo_signal])\n",
    "    elif att_mode == 'news':\n",
    "        att_root = getAttention_new(hidden_dim=hidden_dim,\n",
    "                                    bert_dim=bert_dim,\n",
    "                                    tweet_dim=tweet_dim)(\n",
    "                                        [root_tweet, exo_signal])\n",
    "    else:\n",
    "        raise (AttributeError,\n",
    "               \"Unrecognized attention mode, use 'tweet' or 'news'\")\n",
    "\n",
    "    att_root = tf.keras.layers.RepeatVector(num_users)(att_root)\n",
    "    full_feature = tf.keras.layers.Concatenate(axis=-1)(\n",
    "        [att_root, int_feature])\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(full_feature)\n",
    "    tf.print(\"normal\", norm_full_feature[0][0], output_stream=sys.stdout)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(norm_full_feature)\n",
    "    return tf.keras.models.Model([features, exo_signal, root_tweet], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETINA Static Mode, no exogenous influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StaticModel_noexo(hidden_dim=64,\n",
    "                      tweet_dim=50,\n",
    "                      feature_dim=616,\n",
    "                      num_users=500,\n",
    "                      dropout=0.3,\n",
    "                      **kwargs):\n",
    "    '''Creates static prediction model. \n",
    "    'num_users' is the largest number of users can be taken (retweeters+followers). \n",
    "    Prediction is done every time step independently. \n",
    "    'feature_dim' is the unnormalized features (used previously for logreg).'''\n",
    "    features = tf.keras.layers.Input(shape=(num_users, feature_dim))\n",
    "    root_tweet1 = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    norm_feature = tf.keras.layers.LayerNormalization()(features)\n",
    "    int_feature = tf.keras.layers.Dense(hidden_dim,\n",
    "                                        activation='relu')(norm_feature)\n",
    "    int_feature = tf.keras.layers.Dropout(dropout)(int_feature)\n",
    "\n",
    "    root_tweet = tf.keras.layers.RepeatVector(num_users)(root_tweet1)\n",
    "    full_feature = tf.keras.layers.Concatenate(axis=-1)(\n",
    "        [root_tweet, int_feature])\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(full_feature)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(norm_full_feature)\n",
    "    return tf.keras.models.Model([features, root_tweet1], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETINA Dynamic Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5M7QVQOpQk2"
   },
   "outputs": [],
   "source": [
    "def TemporalModel1(hidden_dim=64,\n",
    "                   bert_dim=500,\n",
    "                   tweet_dim=50,\n",
    "                   feature_dim=617,\n",
    "                   num_users=70,\n",
    "                   time_steps=7,\n",
    "                   dropout=0.2,\n",
    "                   att_mode='tweet',\n",
    "                   **kwargs):\n",
    "    '''Creates temporal prediction model. 'num_users' is the largest number of\n",
    "    users can be taken (retweeters+followers). Prediction is done for 'time_steps'\n",
    "    times in forward time. 'feature_dim' is the unnormalized features (used previously for\n",
    "    logreg).'''\n",
    "    features = tf.keras.layers.Input(shape=(num_users, time_steps,\n",
    "                                            feature_dim))\n",
    "    exo_signal = tf.keras.layers.Input(shape=(None, bert_dim))\n",
    "    root_tweet = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    norm_feature = tf.keras.layers.LayerNormalization()(features)\n",
    "    int_feature = tf.keras.layers.Dense(hidden_dim,\n",
    "                                        activation='relu')(norm_feature)\n",
    "    int_feature = tf.keras.layers.Dropout(dropout)(int_feature)\n",
    "\n",
    "    if att_mode == 'tweet':\n",
    "        att_root = getAttention(hidden_dim=hidden_dim,\n",
    "                                bert_dim=bert_dim,\n",
    "                                tweet_dim=tweet_dim)([root_tweet, exo_signal])\n",
    "    elif att_mode == 'news':\n",
    "        att_root = getAttention_new(hidden_dim=hidden_dim,\n",
    "                                    bert_dim=bert_dim,\n",
    "                                    tweet_dim=tweet_dim)(\n",
    "                                        [root_tweet, exo_signal])\n",
    "    else:\n",
    "        raise (AttributeError,\n",
    "               \"Unrecognized attention mode, use 'tweet' or 'news'\")\n",
    "\n",
    "    att_root = tf.keras.layers.RepeatVector(num_users * time_steps)(att_root)\n",
    "    att_root = tf.keras.layers.Reshape(\n",
    "        (num_users, time_steps, tweet_dim))(att_root)\n",
    "    full_feature = tf.keras.layers.Concatenate(axis=-1)(\n",
    "        [att_root, int_feature])\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(full_feature)\n",
    "    norm_full_feature = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.GRU(hidden_dim,\n",
    "                            return_sequences=True))(norm_full_feature)\n",
    "    norm_full_feature = tf.keras.layers.Dropout(dropout)(norm_full_feature)\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(norm_full_feature)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(norm_full_feature)\n",
    "\n",
    "    return tf.keras.models.Model([features, exo_signal, root_tweet], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETINA Dynamic Mode, no exogenous influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TemporalModel1_noexo(hidden_dim=64,\n",
    "                         bert_dim=500,\n",
    "                         tweet_dim=50,\n",
    "                         feature_dim=617,\n",
    "                         num_users=250,\n",
    "                         time_steps=7,\n",
    "                         dropout=0.2,\n",
    "                         att_mode='tweet',\n",
    "                         **kwargs):\n",
    "    '''Creates temporal prediction model. 'num_users' is the largest number of\n",
    "    users can be taken (retweeters+followers). Prediction is done for 'time_steps'\n",
    "    times in forward time. 'feature_dim' is the unnormalized features (used previously for\n",
    "    logreg).'''\n",
    "    features = tf.keras.layers.Input(shape=(num_users, time_steps,\n",
    "                                            feature_dim))\n",
    "    root_tweet = tf.keras.layers.Input(shape=(tweet_dim, ))\n",
    "    norm_feature = tf.keras.layers.LayerNormalization()(features)\n",
    "    int_feature = tf.keras.layers.Dense(hidden_dim,\n",
    "                                        activation='relu')(norm_feature)\n",
    "    int_feature = tf.keras.layers.Dropout(dropout)(int_feature)\n",
    "\n",
    "    att_root = tf.keras.layers.RepeatVector(num_users * time_steps)(root_tweet)\n",
    "    att_root = tf.keras.layers.Reshape(\n",
    "        (num_users, time_steps, tweet_dim))(att_root)\n",
    "    full_feature = tf.keras.layers.Concatenate(axis=-1)(\n",
    "        [att_root, int_feature])\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(full_feature)\n",
    "    norm_full_feature = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.GRU(hidden_dim,\n",
    "                            return_sequences=True))(norm_full_feature)\n",
    "    norm_full_feature = tf.keras.layers.Dropout(dropout)(norm_full_feature)\n",
    "    norm_full_feature = tf.keras.layers.LayerNormalization()(norm_full_feature)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(norm_full_feature)\n",
    "\n",
    "    return tf.keras.models.Model([features, root_tweet], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Weight Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKuLjKzA5O8u"
   },
   "outputs": [],
   "source": [
    "def masked_weighted_loss(pos_w=1.5, mask_value=-1.):\n",
    "    '''Returns binary cross entropy loss function.\n",
    "    'mask_value':= value used to pad ground truth tensor.\n",
    "    'pos_w':= weight for positive samples (compute as log(total sample/positive sample))'''\n",
    "    def mbce(labels, logits):\n",
    "        sq_label = tf.squeeze(labels, axis=-1)\n",
    "        masks = tf.cast(tf.math.not_equal(sq_label, mask_value),\n",
    "                        dtype=tf.float32)\n",
    "        positives = tf.cast(tf.math.equal(sq_label, 1.),\n",
    "                            dtype=tf.float32) * pos_w\n",
    "\n",
    "        loss = tf.keras.losses.binary_crossentropy(\n",
    "            labels * tf.expand_dims(masks, axis=-1),\n",
    "            logits * tf.expand_dims(masks, axis=-1))\n",
    "        weights = tf.maximum(positives, masks)\n",
    "        return tf.keras.backend.mean(loss * weights, axis=-1)\n",
    "\n",
    "    return mbce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common feature loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topical influence.\n",
    "with open(os.path.join(data, 'doc2vec_root_static_train.pickle'),\n",
    "          'rb') as handle:\n",
    "    doc2_vec_train = pickle.load(handle)\n",
    "with open(os.path.join(data, 'doc2vec_root_static_test.pickle'),\n",
    "          'rb') as handle:\n",
    "    doc2_vec_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exogenous news influence.\n",
    "# NOTE: for news_abalation change the file names according to capture size 15,30,45,60.\n",
    "# Default news size is 60 per tweet.\n",
    "with open(os.path.join(data, \"news2vec_static_train.npy\"), \"rb\") as f:\n",
    "    news2_vec_train = np.load(f)\n",
    "with open(os.path.join(data, \"news2vec_static_test.npy\"), \"rb\") as f:\n",
    "    news2_vec_test = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Feature loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-test features.\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data,\n",
    "                 'train_dynamic_neural_shuffle_3057_70_' + name_ext + '.h5'),\n",
    "    'r')\n",
    "train_features = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "h5f = h5py.File(\n",
    "    os.path.join(data,\n",
    "                 'test_dynamic_neural_shuffle_765_100_' + name_ext + '.h5'),\n",
    "    'r')\n",
    "test_features = h5f['dataset_1'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-test target labels.\n",
    "with open('dynamic_neural_labels_temp_shuffle_new_' + name_ext + '.pickle',\n",
    "          'rb') as handle:\n",
    "    train_labels = pickle.load(handle)\n",
    "with open(\n",
    "        'dynamic_neural_labels_temp_test_shuffle_new_' + name_ext + '.pickle',\n",
    "        'rb') as handle:\n",
    "    test_labels = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATIC Features Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-test features.\n",
    "h5f = h5py.File('train_static_neural_shuffle_3057_200_' + name_ext + '.h5',\n",
    "                'r')\n",
    "train_features = h5f['dataset_1'][:]\n",
    "h5f.close()\n",
    "h5f = h5py.File('test_static_neural_shuffle_765_500_' + name_ext + '.h5', 'r')\n",
    "test_features = h5f['dataset_1'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-test labels.\n",
    "with open('static_neural_labels_temp_shuffle_new_' + news_ext + '.pickle',\n",
    "          'rb') as handle:\n",
    "    train_labels = pickle.load(handle)\n",
    "with open('static_neural_labels_temp_test_shuffle_new_' + news_ext + '.pickle',\n",
    "          'rb') as handle:\n",
    "    test_labels = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores[\"config\"] = {}\n",
    "scores[\"evaluation\"] = {}\n",
    "scores[\"config\"][\"model\"] = \"Dynamic\"  # SET to Static for static\n",
    "scores[\"config\"][\"opt_type\"] = \"SGD\"  # SET to ADAM for static\n",
    "scores['config'][ablation_feature] = int(ablation_value)\n",
    "# scores[\"config\"][\"history\"]=int(history)\n",
    "# scores[\"config\"][\"max_feature_length\"] = int(max_feature_length)\n",
    "\n",
    "att_mode = \"news\"  # or tweets\n",
    "activation = \"relu\"\n",
    "i = 2.5  ## SET to 2 for static\n",
    "n_batch = 32  ## SET to 16 for static\n",
    "n_epoch = 10\n",
    "\n",
    "scores[\"config\"][\"att_mode\"] = att_mode\n",
    "scores[\"config\"][\"activation\"] = activation\n",
    "scores[\"config\"][\"i\"] = i\n",
    "scores[\"config\"][\"n_batch\"] = n_batch\n",
    "scores[\"config\"][\"n_epoch\"] = n_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)  # FOR DYNAMIC\n",
    "# opt='adam' # FOR STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (np.count_nonzero(train_labels.flatten() == 1))\n",
    "pos_w = math.log(len(train_labels.flatten()) / pos)\n",
    "print(pos_w)\n",
    "\n",
    "train_model = TemporalModel1(\n",
    "    num_users=70)  #Static or non-exogenous variants called similarly.\n",
    "train_model.compile(loss=masked_weighted_loss(pos_w * i), optimizer=opt)\n",
    "\n",
    "_ = train_model.fit([train_features, news2_vec_train, doc2_vec_train],\n",
    "                    train_labels,\n",
    "                    batch_size=n_batch,\n",
    "                    epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model and predict values.\n",
    "train_weights = train_model.get_weights()\n",
    "test_model = TemporalModel1(num_users=100)\n",
    "test_model.set_weights(train_weights)\n",
    "y_new = test_model.predict([test_features, news2_vec_test, doc2_vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, pred, k):\n",
    "    # Function to calculate Average Precision @K, used in MAP.\n",
    "    predicted = np.argsort(pred)[-k:][::-1]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def hitsk(actual, pred, k=1):\n",
    "    # Function to calculate hits @ k.\n",
    "    predicted = np.argsort(pred)[-k:]\n",
    "    aucc = 0\n",
    "    for i in predicted:\n",
    "        if i in actual:\n",
    "            aucc += 1\n",
    "    return aucc / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [1, 5, 10, 20, 50,\n",
    "          100]  # Calculate MAP and HITS for various values of k.\n",
    "print(\"\\n\")\n",
    "for k in k_list:\n",
    "    map_ = 0\n",
    "    c = 0\n",
    "    for i in range(765):\n",
    "        q = set(test_labels[i].flatten().nonzero()[0])\n",
    "        if q:\n",
    "            map_ += apk(q, y_new[i].flatten(), k)\n",
    "            c += 1\n",
    "    map_ = map_ / c\n",
    "    print(\"MAP@{} = {}\".format(k, map_))\n",
    "    scores[\"evaluation\"][\"map@{}\".format(k)] = map_\n",
    "\n",
    "print(\"\\n\")\n",
    "for k in [1, 5, 10, 20, 50, 100]:\n",
    "    hit_ = 0\n",
    "    c = 0\n",
    "    for i in range(765):\n",
    "        q = set(test_labels[i].flatten().nonzero()[0])\n",
    "        if q:\n",
    "            hit_ += hitsk(q, y_new[i].flatten(), k)\n",
    "            c += 1\n",
    "    hit_ = hit_ / c\n",
    "    print(\"HITS@{} = {}\".format(k, hit_))\n",
    "    scores[\"evaluation\"][\"hits@{}\".format(k)] = hit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_actual = []\n",
    "for i in range(765):\n",
    "    p_temp = y_new[i].flatten()\n",
    "    a_temp = test_labels[i].flatten()\n",
    "    y_actual.extend(a_temp)\n",
    "    for each in p_temp:\n",
    "        if each < 0.5:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "    assert len(y_pred) == len(y_actual)\n",
    "assert len(y_pred) == len(y_actual)\n",
    "\n",
    "print(classification_report(y_actual, y_pred))\n",
    "scores[\"evaluation\"][\"classification_report\"] = classification_report(\n",
    "    y_actual, y_pred, output_dict=True)\n",
    "\n",
    "roc_auc = roc_auc_score(y_actual, y_pred)\n",
    "print(\"ROC_AUC = {}\".format(roc_auc))\n",
    "scores[\"evaluation\"][\"roc_auc\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        os.path.join(\n",
    "            data, 'retweet_pred_dynamic_' + ablation_feature + '_' +\n",
    "            ablation_value + '.json'), 'w') as f:\n",
    "    json.dump(scores, f, indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RetweetPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
